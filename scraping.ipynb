{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL CODE\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "f = csv.writer(open('z-artist-names.csv', 'w'))\n",
    "f.writerow(['Name', 'Link'])\n",
    "\n",
    "pages = []\n",
    "\n",
    "# Collecting and Parsing a Web Page\n",
    "for i in range(1, 5):\n",
    "    url = 'https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ' + str(i) + '.htm'\n",
    "    pages.append(url)\n",
    "\n",
    "\n",
    "for item in pages:\n",
    "    page = requests.get(item)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Remove bottom links\n",
    "    last_links = soup.find(class_='AlphaNav')\n",
    "    last_links.decompose()\n",
    "\n",
    "    artist_name_list = soup.find(class_='BodyText')\n",
    "    artist_name_list_items = artist_name_list.find_all('a')\n",
    "\n",
    "    for artist_name in artist_name_list_items:\n",
    "        names = artist_name.contents[0]\n",
    "        links = 'https://web.archive.org' + artist_name.get('href')\n",
    "\n",
    "        f.writerow([names, links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING URL DATA\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Write ke file csv\n",
    "f = csv.writer(open('best-novel.csv', 'w', newline=''))\n",
    "f.writerow(['Title', 'Link'])\n",
    "\n",
    "pages = []\n",
    "\n",
    "# Collecting & parsing konten Web\n",
    "for i in range(1, 5):\n",
    "    url = 'https://www.goodreads.com/list/show/67567.Novel_Indonesia_Terbaik?page=' + str(i)\n",
    "    pages.append(url)\n",
    "\n",
    "for item in pages:\n",
    "    page = requests.get(item)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Find elemen class bookTitle di dalam class tableList\n",
    "    novel_title_list = soup.find(class_='tableList')\n",
    "    novel_title_list_items = novel_title_list.find_all(class_='bookTitle')\n",
    "    \n",
    "    # Get masing-masing title dan url dari class tableList\n",
    "    for novel_title in novel_title_list_items:\n",
    "        title = novel_title.find('span',{'itemprop':'name'}).get_text()\n",
    "        link = 'https://www.goodreads.com' + novel_title.get('href')\n",
    "\n",
    "        f.writerow([title, link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING REVIEW DATA FROM DIRECT URL\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Write ke file csv\n",
    "f = csv.writer(open('data-review.csv', 'w', newline=''))\n",
    "f.writerow(['Nama', 'Ulasan'])\n",
    "\n",
    "pages = []\n",
    "\n",
    "url = 'https://www.goodreads.com/book/show/1362193.Laskar_Pelangi' + '?language_code=id'\n",
    "pages.append(url)\n",
    "\n",
    "# Collecting & parsing konten Web\n",
    "for item in pages:\n",
    "    page = requests.get(item)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    \n",
    "    # Find elemen class review di dalam id bookReviews\n",
    "    novel_review_list = soup.find('div', {'id': 'bookReviews'})\n",
    "    novel_review_list_items = novel_review_list.find_all(class_='review')\n",
    "\n",
    "    # Get masing-masing name dan review dari id bookReviews\n",
    "    for novel_review in novel_review_list_items:\n",
    "        name = novel_review.find(class_='user').get_text()\n",
    "        review = novel_review.find(class_='readable').find('span', recursive=False).get_text()\n",
    "\n",
    "        f.writerow([name, review])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING URL ADDRESS\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Write ke file csv\n",
    "csv_output = csv.writer(open('best-novel-url.csv', 'w', newline=''))\n",
    "\n",
    "pages = []\n",
    "\n",
    "# Collecting & parsing konten Web hlm 1-6\n",
    "for i in range(1, 7):\n",
    "    url = 'https://www.goodreads.com/list/show/67567.Novel_Indonesia_Terbaik?page=' + str(i)\n",
    "    pages.append(url)\n",
    "\n",
    "for item in pages:\n",
    "    page = requests.get(item)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Find elemen class bookTitle di dalam class tableList\n",
    "    novel_title_list = soup.find(class_='tableList')\n",
    "    novel_title_list_items = novel_title_list.find_all(class_='bookTitle')\n",
    "    \n",
    "    # Get masing-masing title dan url dari class tableList\n",
    "    for novel_title in novel_title_list_items:\n",
    "        link = 'https://www.goodreads.com' + novel_title.get('href') + '?language_code=id'\n",
    "\n",
    "        csv_output.writerow([link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING REVIEW DATA FROM CSV FILE\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "#Read input url dari file csv & write output review\n",
    "with open('best-novel-url.csv', newline='') as f_urls, open('data-review.csv', 'w', newline='', encoding=\"utf-8\") as f_output:\n",
    "    csv_urls = csv.reader(f_urls)\n",
    "    csv_output = csv.writer(f_output)\n",
    "    csv_output.writerow(['Nama', 'Ulasan', 'Rating'])\n",
    "    \n",
    "    # Collecting & parsing konten web\n",
    "    for line in csv_urls:\n",
    "        r = requests.get(line[0]).text\n",
    "        soup = BeautifulSoup(r, 'lxml')\n",
    "   \n",
    "        # Find elemen class review di dalam id bookReviews\n",
    "        novel_review_list = soup.find('div', {'id': 'bookReviews'})\n",
    "        novel_review_list_items = novel_review_list.find_all(class_='review')\n",
    "        \n",
    "        # Get masing-masing nama & review dari class bookReviews\n",
    "        for novel_review in novel_review_list_items:\n",
    "            name = novel_review.find(class_='user').get_text()\n",
    "            review = novel_review.find(class_='readable').find('span', recursive=False).get_text()\n",
    "            rating_element = novel_review.find('span', {'size': '15x15'})\n",
    "            # Skip item review ketika elemen rating tidak ditemukan\n",
    "            if rating_element == None:                \n",
    "                continue\n",
    "            else :\n",
    "                rating = rating_element.get_text()\n",
    "\n",
    "            csv_output.writerow([name, review, rating])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
